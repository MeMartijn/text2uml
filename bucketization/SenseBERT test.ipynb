{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/martijn/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import tensorflow.compat.v1 as tf\n",
    "import numpy as np\n",
    "\n",
    "sys.path.insert(1, '../data/models/sensebert/code/sense-bert')\n",
    "tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sensebert import SenseBert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is not a known model. Assuming ../data/models/sensebert/models/sensebert-large-uncased is a path or a url...\n",
      "INFO:tensorflow:Restoring parameters from ../data/models/sensebert/models/sensebert-large-uncased/variables/variables\n",
      "This is not a known tokenizer. Assuming ../data/models/sensebert/models/sensebert-large-uncased is a path or a url...\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as session:\n",
    "    sensebert_model = SenseBert('../data/models/sensebert/models/sensebert-large-uncased', session=session)\n",
    "    input_ids, input_mask = sensebert_model.tokenize(\"A customer has a first name and a last name\")\n",
    "    model_outputs = sensebert_model.run(input_ids, input_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ -9.267244 ,  -6.724872 ,  -7.8638334, ...,  -8.289959 ,\n",
       "          -8.120105 ,  -8.000432 ],\n",
       "        [-10.944792 ,  -8.46945  , -10.475722 , ..., -10.359467 ,\n",
       "          -9.940988 ,  -8.4788265],\n",
       "        [ -9.78949  ,  -9.182749 , -12.0799055, ...,  -9.186542 ,\n",
       "         -10.48284  , -11.559656 ],\n",
       "        ...,\n",
       "        [-11.5844345, -10.777775 , -12.140733 , ..., -11.444624 ,\n",
       "         -12.747531 , -13.529133 ],\n",
       "        [-10.475351 ,  -8.881869 , -12.81556  , ..., -10.596663 ,\n",
       "         -10.343219 , -14.975606 ],\n",
       "        [ -7.6808615,  -9.919212 ,  -9.651059 , ..., -10.310689 ,\n",
       "          -8.940258 ,  -8.158318 ]]], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlm_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[unused17]']\n"
     ]
    }
   ],
   "source": [
    "print(sensebert_model.tokenizer.convert_ids_to_tokens([np.argmax(supersense_logits[0][1])]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensebert_model.tokenizer.wordpiece_tokenizer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "contextualized_embeddings, mlm_logits, supersense_logits = model_outputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['noun.group']\n"
     ]
    }
   ],
   "source": [
    "print(sensebert_model.tokenizer.convert_ids_to_senses([np.argmax(supersense_logits[0][5])]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 12, 45)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "supersense_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['adj.all']\n",
      "['noun.person']\n",
      "['adj.all']\n"
     ]
    }
   ],
   "source": [
    "for i, id_ in enumerate(input_ids[0]):\n",
    "  print(sensebert_model.tokenizer.convert_ids_to_senses([np.argmax(supersense_logits[0][i])]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 10, 1024)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contextualized_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len('The store was closed.'.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nlp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-f311c5efb65b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'went'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'nlp' is not defined"
     ]
    }
   ],
   "source": [
    "nlp('went')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['metrology',\n",
       " 'radio',\n",
       " 'number',\n",
       " 'chemistry',\n",
       " 'gastronomy',\n",
       " 'biology',\n",
       " 'chemistry',\n",
       " 'chemistry',\n",
       " 'genetics',\n",
       " 'biochemistry',\n",
       " 'electronics',\n",
       " 'electricity',\n",
       " 'electrotechnology',\n",
       " 'metrology',\n",
       " 'medicine']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy_wordnet.wordnet_annotator import WordnetAnnotator \n",
    "\n",
    "# Load an spacy model (supported models are \"es\" and \"en\") \n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "nlp.add_pipe(\"spacy_wordnet\", after='tagger', config={'lang': nlp.lang})\n",
    "\n",
    "token = nlp('A customer has a name.')[3]\n",
    "\n",
    "# wordnet object link spacy token with nltk wordnet interface by giving acces to\n",
    "# synsets and lemmas \n",
    "# token._.wordnet.synsets()\n",
    "\n",
    "# token._.wordnet.lemmas()\n",
    "token._.wordnet.wordnet_domains()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
